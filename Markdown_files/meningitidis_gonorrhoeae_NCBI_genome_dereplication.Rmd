---
title: "Decontamination and dereplication of all N. gonorrhoeae and N. meningitidis genomes available on NCBI"
author: "J. J. Giacomini"
date: "2023-09-06"
output: html_document
---

The purpose of this document is to detail the processing of all N. gonorrhoeae and N. meningitidis RefSeq genomes (n = 3317), including the decontamination and dereplication based on a 98% Average Nucleotide Identity threshold. 

# 1. Generate metadata.

First we need to generate a metadat file that contains the GenBank assembly IDs for each genome.

```{r filter genomes by target taxa}
library(dplyr)
unfiletred_Neisseriaceae_df <- read.csv("/Users/home/SPECIES_LEVEL_PANGENOMES/Neisseriaceae/DATA/Temps/Neisseriaceae_NCBI_metadata_un_filtered.csv", header = TRUE)


target_taxa <- unfiletred_Neisseriaceae_df %>% 
  filter(Genus == "Neisseria" & Species == "meningitidis" | Species == "gonorrhoeae")

meningitidis_gonorrhoeae_assembly_list <- target_taxa %>% 
  select(Assembly) 

write.table(meningitidis_gonorrhoeae_assembly_list, "/Users/home/SPECIES_LEVEL_PANGENOMES/Neisseriaceae/DATA/meningitidis_gonorrhoeae_GCA_assembly_IDs.txt", quote = FALSE, row.names = FALSE, col.names = FALSE)

```
We then generated a metadata file for the slected genomes. We did so by filtering the previously made Genbank/RefSeq metadata file to inlcude only the GenBank assemblies of interest.  

```{bash Filter Genbank metadata by selected NCBI genomes}
# set working directory
DIR_DATA=/Users/home/SPECIES_LEVEL_PANGENOMES/Neisseriaceae/DATA

# get original header
head -n 2 $DIR_DATA/assembly_summary_genbank.txt | tail -n 1 > $DIR_DATA/genbank_header.txt

# make file
cp $DIR_DATA/genbank_header.txt $DIR_DATA/meningitidis_gonorrhoeae_genbank_metadata.txt

# Use grep to filter the rows of file1.txt based on the content of file2.txt using the following command:
grep -Ff $DIR_DATA/meningitidis_gonorrhoeae_GCA_assembly_IDs.txt $DIR_DATA/assembly_summary_genbank.txt >> $DIR_DATA/meningitidis_gonorrhoeae_genbank_metadata.txt


for genome in `cat $DIR_DATA/meningitidis_gonorrhoeae_GCA_assembly_IDs.txt`
do
grep -e "$genome" $DIR_DATA/assembly_summary_genbank.txt >> $DIR_DATA/meningitidis_gonorrhoeae_genbank_metadata.txt
done

#cat $DIR_DATA/meningitidis_gonorrhoeae_genbank_metadata.txt | wc -l

# convert genbank metadata txt format to csv format
tr '\t' ',' < $DIR_DATA/meningitidis_gonorrhoeae_genbank_metadata.txt > $DIR_DATA/meningitidis_gonorrhoeae_genbank_metadata.csv

```

We then manually filtered out genomes, using Excel, to remove those excluded from refseq due to issues highlighted in genbank metadata.If a genome had the following notes it was removed:

•	derived from metagenome - the genomic sequence was assembled from metagenomic sequencing rather than a pure culture leading to concerns about the accuracy of organism assignment and possible cross-contamination.

•	genome length too large - total non-gapped sequence length of the assembly is more than 1.5 times that of the average for the genomes in the Assembly resource from the same species, more than 15 Mbp, or is otherwise suspiciously long.

•	genome length too small - total non-gapped sequence length of the assembly is less than half that of the average for the genomes in the Assembly resource from the same species, less than 300 Kbp, or is otherwise suspiciously short.

•	many frameshifted proteins - the CDSs predicted by the NCBI Prokaryotic Genome Annotation Pipeline have a suspiciously high number of frameshifts. For any clade containing at least 10 good quality assemblies the cutoff is more than three standard deviations from average or 5% of annotated CDSs, whichever is larger. For any clade containing less than 10 good quality assemblies the cutoff is more than 30% of total CDSs.


# 2. Download, reformat and dereoplicate genomes

Now that we have the metadata file, we can proceed with downloading, reformatting and derepliction. First we need to send the metadata file to the MBL server. 

```{bash send materials to MBL server}

# upload NCBI metadata file onto the server (Run on local machine)
scp -r /Users/home/SPECIES_LEVEL_PANGENOMES/Neisseriaceae/DATA/filtered_meningitidis_gonorrhoeae_genbank_metadata.csv jgiacomini@evol5.mbl.edu:/workspace/jmarkwelchlab/P_0003_Neisseriaceae/DATA/filtered_meningitidis_gonorrhoeae_genbank_metadata.csv

# send script to download, process and dereplicate men/gno genomes
scp -r /Users/home/SPECIES_LEVEL_PANGENOMES/Neisseriaceae/SCRIPTS/Men_Gno_download_process_dereplication.sh jgiacomini@evol5.mbl.edu:/workspace/jmarkwelchlab/P_0003_Neisseriaceae/SCRIPTS/Men_Gno_download_process_dereplication.sh

```


Now we can run the script on MBL server to download, process and dereplicate men/gno genomes. Note the following requirements:

- Must be in project directory P_0003_Neisseriaceae
- Make sure anvio 7.1 environment loaded

```{bash Run script to downlaod and reformat genomes}

clusterize -n 20 -l LOGS/Men_Gno_download_process_dereplication.log -m jgiacomini@forsyth.org /workspace/jmarkwelchlab/P_0003_Neisseriaceae/SCRIPTS/Men_Gno_download_process_dereplication.sh
```


The script failed before dereplication because some expected contigs databases were missing. Inspection of the log file revealed that in total, 3309 genomes out of 3317 downloaded successfully. There is no obvious evidence to explain why some genomes failed to download. 

Here is a list of the failed downloads:

G_2724 G_2725 G_2730 G_3118 G_3123 G_3132 G_3134 G_3138 

I then re-downloaded the missing genomes using the code below.

```{bash download missing genomes form first run}

for ID in G_2724 G_2725 G_2730 G_3118 G_3123 G_3132 G_3134 G_3138
do

grep -e "$ID" $DIR_Data/meningitidis_gonorrhoeae-ncbi_genomes_id.txt | sed -e 's/*._id_//g' >> $DIR_Data/missing_genomes_meningitidis_gonorrhoeae.txt

done
```

The output of the code above gives us assmembly IDs so that we can re-try the downloads:

G_2724    GCA_001141185.1
G_2725    GCA_001150905.1
G_2730    GCA_900036725.1
G_3118    GCA_900453965.1
G_3123    GCA_003855215.1
G_3132    GCA_003855415.1
G_3134    GCA_003855175.1
G_3138    GCA_900454105.1


Now we can make a list with gca path for download; GCA download link is in column 20. And then download the missing genomes, 
```{bash}

for gca_assembly in GCA_001141185.1 GCA_001150905.1 GCA_900036725.1 GCA_900453965.1 GCA_003855215.1 GCA_003855415.1 GCA_003855175.1 GCA_900454105.1
do
cat $DIR_Data/filtered_meningitidis_gonorrhoeae_genbank_metadata.csv | grep -e "$gca_assembly" > tmp.csv
cat tmp.csv | awk -F',' '{print $20}' | awk 'BEGIN{FS=OFS="/"}{print $0,$NF"_genomic.fna.gz"}' | sed -e 's/"//g' >> $DIR_Data/missing_meningitidis_gonorrhoeae-download_url.txt
done

rm tmp.csv

projectID=P_0003_Neisseriaceae
mainDIR=/workspace/jmarkwelchlab
DIR_Data=$mainDIR/$projectID/DATA
DIR_NCBI=$mainDIR/$projectID/01_NCBI_GENOMES
DIR_NCBI_men_gno=$DIR_NCBI/MEN_GNO_GENOMES
for gca_assembly in `cat $DIR_Data/missing_meningitidis_gonorrhoeae-download_url.txt`
do
gcaFile=$( echo $gca_assembly | awk -F'/' '{print $NF}' | sed -e 's/\.gz//' )
if [ ! -f $DIR_NCBI_men_gno/${gcaFile} ]
then
echo "Downloading: ${gcaFile} "
wget -q -P $DIR_NCBI_men_gno $gca_assembly
gunzip $DIR_NCBI_men_gno/${gcaFile}.gz
echo "Successfully unzipped: ${gcaFile} "
fi
done

DIR_Assemblies=$mainDIR/$projectID/02_ASSEMBLIES
DIR_Assemblies_men_gno=$mainDIR/$projectID/02_ASSEMBLIES/MEN_GNO_GENOMES

# copy newly named fasta files
for genomes_id in `cat $DIR_Data/missing_genomes_meningitidis_gonorrhoeae.txt`
do
gca_id=$(echo $genomes_id | awk -F'_id_' '{print $2}')
old_name=$(find $DIR_NCBI_men_gno -name "$gca_id*")
NEW_NAME=$(echo "$genomes_id" | awk -F'_id_' -v new_dir="$DIR_Assemblies_men_gno" '{print new_dir"/"$1"-RAW.fa"}' )
cp $old_name $NEW_NAME
done 


DIR_Contigs=$mainDIR/$projectID/03_GENOMES_EDITED
DIR_Contigs_men_gno=$DIR_Contigs/MEN_GNO_GENOMES
DIR_ContigsDB=$mainDIR/$projectID/04_CONTIGS_DB
DIR_ContigsDB_men_gno=$DIR_ContigsDB/MEN_GNO_GENOMES

#####  reformat genome deflines (edit this script to remove para) and build contigs db and make paths file
for genome in `cat $DIR_Data/missing_genomes_meningitidis_gonorrhoeae.txt`
do
EDITED_GENOMES=$DIR_Assemblies_men_gno/${genome}-RAW.fa
CONTIGS=$DIR_Contigs_men_gno/${genome}.fa
REPORT=$DIR_Contigs_men_gno/${genome}.report.tsv
anvi-script-reformat-fasta -l $minContigSIZE -o $CONTIGS --simplify-names --prefix ${genome} --seq-type NT -r $REPORT $EDITED_GENOMES 
done

# generate anvio contigs databases
for genome in `cat $DIR_Data/missing_genomes_meningitidis_gonorrhoeae.txt`
do
CONTIGS=$DIR_Contigs_men_gno/${genome}.fa
contigsDB=$DIR_ContigsDB_men_gno/${genome}-contigs.db
numThreads=10
anvi-gen-contigs-database -f $CONTIGS -n ${genome} -o $contigsDB -T $numThreads 
done

```


In addition to several genomes failing to download, several contigs databases also failed to generate for unknown reasons. Below is code to re-do the generation of anvio contigs databases for those genomes. 

```{bash Re-do generate contig DBs that failed}

anvi-gen-contigs-database -f $DIR_Contigs_men_gno/G_0175.fa -n G_0175 -o $DIR_ContigsDB_men_gno/G_0175-contigs.db -T 10 --force-overwrite       anvi-gen-contigs-database -f $DIR_Contigs_men_gno/G_2481.fa -n G_2481 -o $DIR_ContigsDB_men_gno/G_2481-contigs.db -T 10 --force-overwrite       
anvi-gen-contigs-database -f $DIR_Contigs_men_gno/G_0325.fa -n G_0325 -o $DIR_ContigsDB_men_gno/G_0325-contigs.db -T 10 --force-overwrite       
anvi-gen-contigs-database -f $DIR_Contigs_men_gno/G_1983.fa -n G_1983 -o $DIR_ContigsDB_men_gno/G_1983-contigs.db -T 10 --force-overwrite      
anvi-gen-contigs-database -f $DIR_Contigs_men_gno/G_2070.fa -n G_2070 -o $DIR_ContigsDB_men_gno/G_2070-contigs.db -T 10 --force-overwrite         
```


We should check to see if all contigs databases were generated correctly.

```{bash}
for genome in `cat $meningitidis_gonorrhoeae_genomesID`
do
contigsDB=$DIR_ContigsDB_men_gno/${genome}-contigs.db
anvi-db-info $contigsDB | tee -a TMP_DB_INFO/Men_gen_contigs_db_info.txt
done

```


At this point all 3317 genomes were successfully downloaded, unzipped, reformatted and contigs databases were generated for each. The next step is dereplication using Anvio and FastANI. Please see the main analysis markdown file for more information. 
